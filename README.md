# ğŸ§ª DCGAN on CelebA (WIP)

> Work in progress â€” training a **Deep Convolutional GAN (DCGAN)** to generate 64Ã—64 human face images using **TensorFlow/Keras**.  
> Built by **Mohammad Hemmat** â€” Computer Engineering student | AI, Robotics & Cybersecurity Enthusiast.

---

## âœ¨ Project Goals
- Implement a reproducible **DCGAN** baseline on the **CelebA** dataset  
- Visualize generated faces during training  
- Save and restore model checkpoints for later continuation  
- Later steps: Add FID metric, better architectures, and stability improvements

---

## ğŸ–‡ï¸ Status
- âœ… Data pipeline + preprocessing  
- âœ… Generator & Discriminator models  
- âœ… Checkpoints and sample saving  
- ğŸŸ¨ Documentation and fine-tuning  
- â­ï¸ Next: Implement evaluation metrics, add configuration system, and enhance generator quality

---

## ğŸ“¦ Environment Setup

### Requirements
- Python 3.10+
- TensorFlow 2.12+ (GPU recommended)
- NumPy
- Matplotlib
- Pillow
- ImageIO

### Installation
```bash
# Clone the repository
git clone https://github.com/<your-username>/dcgan-celeba.git
cd dcgan-celeba

# Create a virtual environment
python -m venv .venv
# Activate
# Windows:
.venv\Scripts\activate
# macOS/Linux:
source .venv/bin/activate

# Install dependencies
pip install -r requirements.txt
```


## ğŸ§  Model Overview

### ğŸ§© Generator
The generator transforms 100-dimensional random noise vectors into 64Ã—64 RGB images using a series of transposed convolutional layers.

**Architecture Summary**
| Layer | Output Shape | Notes |
|-------|---------------|-------|
| Dense | (8Ã—8Ã—256) | Initial projection |
| Reshape | (8, 8, 256) | Start of upsampling |
| Conv2DTranspose (128) | (8, 8, 128) | Upsampling |
| Conv2DTranspose (64) | (16, 16, 64) | Upsampling |
| Conv2DTranspose (32) | (32, 32, 32) | Upsampling |
| Conv2DTranspose (3, tanh) | (64, 64, 3) | Output image |

---

### ğŸ§© Discriminator
The discriminator distinguishes between **real** and **generated** images using convolutional layers with LeakyReLU activations and dropout for regularization.

**Architecture Summary**
| Layer | Output Shape | Notes |
|-------|---------------|-------|
| Conv2D (64) | (32, 32, 64) | Feature extraction |
| Conv2D (128) | (16, 16, 128) | Deeper feature mapping |
| Conv2D (256) | (8, 8, 256) | Compact representation |
| Flatten + Dense(1) | - | Real/Fake classification output |

---

### âš™ï¸ Training Configuration
- **Loss Functions:** Binary cross-entropy (from logits)  
- **Optimizers:** Adam (learning rate = 1e-4)  
- **Batch Size:** 128  
- **Epochs:** 50 (default)  
- **Noise Dimension:** 100  
- **Normalization:** Image pixel values scaled to [-1, 1]
## ğŸ“¸ Results

> Below are sample outputs generated by the DCGAN during training.  
> Images will be updated as training progresses and model quality improves.

| Epoch | Sample Output |
|:-----:|:---------------:|
| 1 | ![epoch1](samples/epoch_0001.png) |
| 10 | ![epoch10](samples/epoch_0010.png) |
| 25 | ![epoch25](samples/epoch_0025.png) |
| 50 | ![epoch50](samples/epoch_0050.png) |

*(Replace the image file names with your actual outputs.  
Each image should be saved automatically in the `samples/` folder during training.)*

---

### ğŸ“ˆ Observations
- As training progresses, generated faces gradually gain sharper and more realistic features.  
- Early epochs typically show blurred or noisy images.  
- Mid to late epochs (around 30â€“50) begin to capture facial structure, symmetry, and color distribution.  
- Training stability and quality can be improved by adjusting:
  - Learning rates of the generator and discriminator
  - Batch normalization placement
  - Training ratio (e.g., multiple discriminator updates per generator update)

---

### ğŸ§® Future Improvements
- [ ] Integrate **FID** and **Inception Score** metrics for quantitative evaluation  
- [ ] Add **TensorBoard** visualizations  
- [ ] Try **WGAN-GP** or **Spectral Normalization** for stability  
- [ ] Explore higher-resolution outputs (128Ã—128 or 256Ã—256)  
- [ ] Experiment with **progressive growing GANs**
## ğŸ§ª Roadmap

This section outlines planned improvements and future experiments for the DCGAN project.

### ğŸ¯ Short-Term Goals
- [ ] Save generated image grids after **every epoch** automatically  
- [ ] Add **command-line arguments (CLI)** for dataset path, epochs, and batch size  
- [ ] Integrate **TensorBoard** for live visualization of losses and generated samples  
- [ ] Add **requirements.txt** auto-check for dependencies  
- [ ] Clean up unused imports and paths

### ğŸš€ Mid-Term Goals
- [ ] Implement **FID (FrÃ©chet Inception Distance)** and **Inception Score** evaluation  
- [ ] Experiment with **different loss functions** (WGAN-GP, LSGAN, etc.)  
- [ ] Add **Spectral Normalization** to the discriminator for training stability  
- [ ] Create a **config file (YAML/JSON)** for training parameters  
- [ ] Add automatic **sample saving** to `/samples/` directory per epoch

### ğŸ§  Long-Term Goals
- [ ] Implement **Progressive Growing GAN** for higher-resolution face generation  
- [ ] Train and benchmark with **CelebA-HQ** dataset  
- [ ] Experiment with **StyleGAN-like architectures**  
- [ ] Convert the project into a modular **GAN training framework**  
- [ ] Publish trained model weights and interactive notebook demo

---

### ğŸ Milestone Tracking
| Milestone | Status | Description |
|------------|:------:|-------------|
| Baseline DCGAN Training | âœ… | Working implementation on 64Ã—64 CelebA images |
| Checkpointing + Sample Saving | âœ… | Added training checkpoints and grid saving |
| TensorBoard Integration | â³ | In progress |
| Evaluation Metrics (FID/IS) | â­ï¸ | Planned |
| Advanced Architectures (WGAN-GP/StyleGAN) | ğŸ”œ | Future work |

---

> _â€œGreat models arenâ€™t built overnight â€” they evolve through experimentation, curiosity, and iteration.â€_

## âš–ï¸ License

This project is open-source under the **MIT License** â€” see the [`LICENSE`](LICENSE) file for full details.

**Note:**  
The **CelebA dataset** is *not included* and remains under its own license.  
Please follow the [CelebA Terms of Use](http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html) if you plan to use it.

---

## ğŸ‘¨â€ğŸ’» Author

**Mohammad Hemmat**  
ğŸ“ Scholarship Student at **Peter the Great St. Petersburg Polytechnic University, Russia**  
ğŸ’» Computer Engineering Student | AI & Robotics Enthusiast | Cybersecurity Explorer  

**Connect with me:**
- [![Telegram](https://img.shields.io/badge/Telegram-Contact-blue?logo=telegram)](https://t.me/Astr0Mh)
- [![Telegram Channel](https://img.shields.io/badge/Telegram-Channel-lightblue?logo=telegram)](https://t.me/TheThoughtsDirectory)
- [![LinkedIn](https://img.shields.io/badge/LinkedIn-Profile-blue?logo=linkedin)](https://www.linkedin.com/in/mohammadhemmat/)

---

## ğŸ’¬ Acknowledgments

- **CelebA Dataset** â€“ for providing a high-quality dataset for face generation  
- **TensorFlow/Keras Team** â€“ for the excellent deep learning framework  
- **Community contributors** â€“ for open-source GAN implementations that inspired this work  

---

> _â€œThe future belongs to those who build it with code, creativity, and courage.â€_  
> â€” **Mohammad Hemmat**

---

### â­ Support

If you find this project useful or interesting, consider giving it a **â­ star** on GitHub â€” it helps others find it and supports further development.

